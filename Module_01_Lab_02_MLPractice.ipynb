{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravisankarnaidu/FMML_Projects_Labs/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3430fc27-9212-468d-d084-3d8855ec25aa"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f56000-b76b-4f9e-de94-e34258e26fc9"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970756d0-9ffe-4358-ebb3-2be7993757d0"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8092c88e-a038-471b-e972-1d74eb1438e7"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be4fa54-9482-483d-ee19-76e67952e5dc"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b91a2231-efc9-48a2-b161-4d87c1288c51"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1)*The size of the validation set in a machine learning experiment can have a significant impact on the accuracy and generalization of the model. Let's explore how changing the percentage of the validation set can affect the accuracy:\n",
        "\n",
        "**Increasing the Percentage of the Validation Set:**\n",
        "\n",
        "1. **Pros:**\n",
        "   - **Higher Confidence in Validation Results:** A larger validation set provides a more reliable estimate of how well your model will generalize to unseen data. This can give you higher confidence in the reported validation accuracy.\n",
        "\n",
        "2. **Cons:**\n",
        "   - **Smaller Training Set:** As you increase the validation set size, the training set size decreases proportionally. Smaller training sets may lead to models that underfit the data because they have less data to learn from.\n",
        "   - **Risk of Overfitting on Validation Set:** If the validation set is too large relative to the training set, the model might start overfitting to the validation data, leading to an overly optimistic estimate of its performance.\n",
        "\n",
        "**Reducing the Percentage of the Validation Set:**\n",
        "\n",
        "1. **Pros:**\n",
        "   - **Larger Training Set:** A smaller validation set means a larger training set. This can help the model capture more patterns and generalize better.\n",
        "   - **Faster Model Training:** With a smaller validation set, training iterations can be faster because less time is spent on validation.\n",
        "\n",
        "2. **Cons:**\n",
        "   - **Less Reliable Validation Results:** A smaller validation set may lead to noisier validation accuracy estimates, making it harder to determine how well the model will perform on unseen data.\n",
        "   - **Risk of Overfitting the Training Set:** With a very small validation set, you may not catch overfitting issues early, and the model's performance on unseen data may be worse than indicated by the validation accuracy.\n",
        "\n",
        "In practice, the choice of the validation set size is a trade-off. It depends on factors such as the amount of available data, the complexity of the model, and the need for reliable validation estimates. Cross-validation techniques, such as k-fold cross-validation, can help mitigate some of the issues associated with small validation sets by repeatedly splitting the data into training and validation subsets and averaging the results.\n",
        "\n",
        "Generally, it's advisable to strike a balance between the training and validation set sizes. A typical split might be 70-80% for training and 20-30% for validation. However, the exact split ratio can vary depending on the specific problem and dataset characteristics. It's essential to experiment and tune your model to find the optimal validation set size for your particular scenario."
      ],
      "metadata": {
        "id": "lKwjHH1T-FBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2)*The size of the train and validation sets can have a significant impact on how well you can predict the accuracy on the test set using the validation set. The relationship between these sets is fundamental to the process of training and evaluating machine learning models. Here are some key considerations:\n",
        "\n",
        "1. **Bias in Validation Set Estimate**: The validation set is typically used to estimate the model's performance on unseen data, like the test set. If the validation set is very small compared to the training set, it might not provide a reliable estimate of model performance. A small validation set may introduce bias, leading to overly optimistic or pessimistic performance estimates.\n",
        "\n",
        "2. **Overfitting**: If you have a very small validation set, it becomes easier for the model to overfit to that data. Overfitting occurs when the model learns to perform well on the specific examples in the validation set but doesn't generalize well to new, unseen data. In this case, the validation set may not accurately represent the model's true ability to generalize.\n",
        "\n",
        "3. **Underfitting**: On the other hand, if the validation set is too large relative to the training set, it might not be challenging enough for the model to learn from. This can result in underfitting, where the model fails to capture the underlying patterns in the data. In this scenario, the validation set may give a pessimistic estimate of model performance.\n",
        "\n",
        "4. **Randomness and Variability**: The size of the validation set can also affect the randomness and variability in your performance estimates. Smaller validation sets are more susceptible to fluctuations in performance due to the specific examples chosen, making it harder to trust the results. Larger validation sets tend to provide more stable and reliable estimates.\n",
        "\n",
        "5. **Data Distribution**: The size of the validation set should also be representative of the distribution of the test set. If your test set contains certain patterns or classes in a specific proportion, your validation set should reflect this as well. Otherwise, your performance estimates may not be accurate.\n",
        "\n",
        "6. **Cross-Validation**: In cases where your dataset is small, you might consider techniques like k-fold cross-validation, which involves splitting the data into multiple subsets and repeatedly using each as a validation set. This can provide a more robust estimate of model performance.\n",
        "\n",
        "In summary, the size of the train and validation sets plays a crucial role in predicting the accuracy on the test set. It's important to strike a balance between having enough data in the validation set to make meaningful estimates and ensuring that it is representative of the overall dataset. The choice of split size should be based on the specific characteristics of your dataset and the goals of your machine learning project."
      ],
      "metadata": {
        "id": "aarUHXAy-6IN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3)*The percentage of data to reserve for the validation set can vary depending on the size of your overall dataset and the specific characteristics of your problem. There is no one-size-fits-all answer, but I can provide some general guidelines:\n",
        "\n",
        "1. **Common Split Ratios**: A common practice is to split the data into three sets: a training set, a validation set, and a test set. A typical split might be 70% for training, 15% for validation, and 15% for testing. This is often referred to as a 70-15-15 split. However, these percentages can be adjusted based on your dataset size and other considerations.\n",
        "\n",
        "2. **Small Datasets**: If you have a small dataset (e.g., fewer than 1,000 samples), you may need to allocate a larger percentage to the validation and test sets. For example, an 80-10-10 or 60-20-20 split might be more appropriate. This helps ensure that you have enough data to estimate model performance reliably.\n",
        "\n",
        "3. **Large Datasets**: With very large datasets (e.g., millions of samples), you can afford to allocate a smaller percentage to the validation and test sets, such as 90-5-5 or even 95-2.5-2.5. In such cases, the absolute number of samples in the validation and test sets is still sufficient for meaningful evaluation.\n",
        "\n",
        "4. **Cross-Validation**: In some situations, you may opt for k-fold cross-validation, where the dataset is divided into k subsets, and the model is trained and validated k times, using each subset as the validation set once. This helps to make efficient use of the data and can provide robust performance estimates.\n",
        "\n",
        "5. **Stratified Split**: If your dataset has class imbalance (i.e., certain classes are underrepresented), consider using stratified sampling to ensure that each class is represented proportionally in the validation and test sets. This can be especially important for classification tasks.\n",
        "\n",
        "6. **Iterative Tuning**: You may need to iteratively adjust the split percentages and experiment with different ratios to find the balance that works best for your specific problem and dataset.\n",
        "\n",
        "Ultimately, the key is to strike a balance between having a sufficiently large validation set to estimate model performance accurately and maximizing the amount of data available for training. The choice of split ratios should be guided by your domain knowledge, the size of your dataset, and the goals of your machine learning project."
      ],
      "metadata": {
        "id": "gz_oCONT_8Sv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f788d2-c2e6-47f5-a0dd-2f0482c0b09f"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1)*Yes, averaging the validation accuracy across multiple splits can indeed provide more consistent and reliable results when assessing the performance of a machine learning model. This technique is commonly known as cross-validation, and it helps to address the potential variability in model performance that can arise from a single train-validation split. Here's how it works:\n",
        "\n",
        "1. **K-Fold Cross-Validation**: The most common form of cross-validation is k-fold cross-validation. In this method, the dataset is divided into k subsets or \"folds\" of approximately equal size. The model is then trained and validated k times, with each fold serving as the validation set once while the remaining k-1 folds are used for training. This process yields k different validation accuracy scores.\n",
        "\n",
        "2. **Averaging Results**: After each of the k iterations, you have k different validation accuracy scores. These scores can be averaged to obtain a single, more robust estimate of the model's performance. This average is often considered a more stable and reliable measure of performance compared to a single validation set.\n",
        "\n",
        "3. **Reduced Variability**: Cross-validation helps reduce the impact of randomness and variability that can occur when splitting data into a single train-validation set. It provides a better representation of the model's generalization performance across different subsets of the data.\n",
        "\n",
        "4. **Hyperparameter Tuning**: Cross-validation is particularly useful during hyperparameter tuning (e.g., tuning the learning rate, regularization strength, or model architecture). It allows you to assess how different hyperparameters impact model performance across multiple data splits, helping you make more informed choices.\n",
        "\n",
        "5. **Model Selection**: Cross-validation can also be used for model selection. You can compare the cross-validated performance of different models to determine which one performs best on average across various data partitions.\n",
        "\n",
        "6. **Stratified Cross-Validation**: To ensure that each class in a classification problem is represented in the validation sets proportionally, you can use stratified k-fold cross-validation.\n",
        "\n",
        "Overall, cross-validation is a valuable technique for obtaining more stable and consistent estimates of a model's performance. It is especially useful when working with limited data or when you want to make more confident assertions about how well your model is likely to perform on unseen data. Common values for k include 5-fold and 10-fold cross-validation, but the choice of k can depend on the size of your dataset and computational resources."
      ],
      "metadata": {
        "id": "j5bTtSTfAhp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2)*Cross-validation, such as k-fold cross-validation, provides a more accurate estimate of test accuracy compared to a single train-validation split when you have a limited dataset. However, it's important to understand the distinction between \"test accuracy\" and \"cross-validated accuracy.\"\n",
        "\n",
        "1. **Test Accuracy**: The \"test accuracy\" typically refers to the performance of your model on a completely independent and unseen dataset that is separate from your training and validation data. It is the ultimate measure of how well your model generalizes to new, real-world data.\n",
        "\n",
        "2. **Cross-Validated Accuracy**: Cross-validated accuracy is an estimate of your model's performance based on multiple rounds of training and validation on different subsets of your training data. It provides a more stable and reliable estimate of how your model is likely to perform on unseen data compared to a single validation split.\n",
        "\n",
        "Here's how cross-validation relates to test accuracy:\n",
        "\n",
        "- Cross-validated accuracy is a valuable metric for assessing your model's performance and comparing different models during development and hyperparameter tuning. It helps you make informed decisions about model selection and tuning.\n",
        "\n",
        "- While cross-validated accuracy provides a better estimate of a model's generalization performance compared to a single validation split, it is still an estimate based on your training data. It does not replace the need for a true test set to evaluate how well your model performs in the real world.\n",
        "\n",
        "- The true test accuracy, measured on a separate and previously unseen dataset, is what ultimately tells you how your model is expected to perform in production or on new data. Cross-validated accuracy is a useful intermediate step to assess your model's capabilities and guide your development process, but it may not precisely match the test accuracy.\n",
        "\n",
        "In summary, cross-validation does not replace the need for a dedicated test set but provides a more accurate and reliable estimate of your model's performance compared to a single validation split. It is a crucial tool for model development and evaluation, especially when you have limited data. To get the true test accuracy, you should reserve a separate test dataset that is not used during model development or hyperparameter tuning."
      ],
      "metadata": {
        "id": "5LNyqadwAiyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3)*The number of iterations or folds in cross-validation can have an effect on the accuracy estimate, and the impact depends on various factors. Generally, increasing the number of iterations (i.e., using more folds) can provide a more stable and robust estimate of a model's performance, but there are some trade-offs to consider:\n",
        "\n",
        "**Pros of Using More Iterations:**\n",
        "\n",
        "1. **Stability**: With more iterations, the estimate of model performance becomes more stable and less sensitive to the specific random split of the data. This can help reduce the variability in your performance estimate.\n",
        "\n",
        "2. **Better Utilization of Data**: More iterations allow you to make better use of your available data for both training and validation. This can be especially important when you have a limited dataset.\n",
        "\n",
        "3. **Improved Hyperparameter Tuning**: When conducting hyperparameter tuning, more iterations provide a more comprehensive assessment of how different hyperparameters affect model performance, leading to better parameter choices.\n",
        "\n",
        "**Cons and Considerations:**\n",
        "\n",
        "1. **Computational Cost**: Using a higher number of iterations increases the computational cost of cross-validation, as you need to train and evaluate the model multiple times. This can be a significant factor when working with large datasets or complex models.\n",
        "\n",
        "2. **Diminishing Returns**: While increasing the number of iterations can improve the estimate's stability, there are diminishing returns. After a certain point, adding more folds may not substantially improve the estimate but will significantly increase the computation time.\n",
        "\n",
        "3. **Imbalanced Data**: In cases where your data is highly imbalanced, using a very high number of folds can lead to issues where some folds contain very few or no instances of certain classes. This can result in biased performance estimates.\n",
        "\n",
        "4. **Small Datasets**: If your dataset is very small, you might need to be cautious about using a large number of folds because it can lead to each fold having too few samples for training, potentially resulting in poor model training.\n",
        "\n",
        "In practice, common choices for the number of iterations in k-fold cross-validation include 5-fold and 10-fold cross-validation. These values strike a balance between stability and computational cost for many datasets. However, the optimal number of iterations can vary depending on your specific dataset and goals.\n",
        "\n",
        "In summary, increasing the number of iterations in cross-validation can lead to a more stable and robust estimate of model performance, but there are trade-offs to consider, including computational cost and potential issues with imbalanced or small datasets. The choice of the number of iterations should be based on practical considerations and the characteristics of your data."
      ],
      "metadata": {
        "id": "aS2N17TtAkfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*4)*Increasing the number of iterations in cross-validation can help mitigate some of the challenges associated with having a very small training dataset or validation dataset to some extent, but it may not completely resolve all issues. Here's how increasing iterations can help in such situations:\n",
        "\n",
        "**Benefits of Increasing Iterations with Small Datasets:**\n",
        "\n",
        "1. **Better Utilization of Data**: With more iterations, you can partition your small dataset into different folds more times. This can help in better utilizing the limited data for training and validation.\n",
        "\n",
        "2. **Stability**: More iterations provide a more stable estimate of model performance, reducing the impact of randomness in the data split. This can make the performance estimate less sensitive to the specific choice of the validation set.\n",
        "\n",
        "3. **Enhanced Hyperparameter Tuning**: If you're conducting hyperparameter tuning, having more iterations can help you explore a wider range of hyperparameters and make more informed decisions about the best configuration.\n",
        "\n",
        "However, there are limitations and considerations:\n",
        "\n",
        "1. **Imbalanced Data**: If your dataset is not only small but also imbalanced (i.e., certain classes have very few samples), increasing the iterations might lead to some folds with extremely imbalanced class distributions. This can affect the reliability of the performance estimate, especially for minority classes.\n",
        "\n",
        "2. **Computational Cost**: Using a high number of iterations can significantly increase computational cost, as each iteration involves training and evaluating the model. This might be a concern if you have limited computational resources or time constraints.\n",
        "\n",
        "3. **Reduced Training Data per Fold**: When you have a very small dataset, increasing the number of iterations means that each fold will have an even smaller portion of the data for training. This can lead to issues with model training and may result in poor model generalization.\n",
        "\n",
        "In cases where you have a very small training dataset, it's important to carefully consider the trade-offs. While increasing the iterations can help, it may not fully compensate for the limitations imposed by a small dataset. It's also worth exploring other techniques to address data scarcity, such as data augmentation, transfer learning, or, if possible, obtaining more data.\n",
        "\n",
        "Ultimately, the choice of the number of iterations should be based on practical constraints, computational resources, and the specific challenges posed by your dataset. If you find that increasing the iterations doesn't sufficiently address the issues related to small datasets, you might need to explore alternative strategies or consider collecting more data if feasible."
      ],
      "metadata": {
        "id": "fIF0pqikAljT"
      }
    }
  ]
}